# -*- coding: utf-8 -*-
"""homework3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FtCIWtvg_lgVPH95iEmcNozW_4Uzh2kL

**HOMEWORK 3**
"""

import gzip
from collections import defaultdict
import math
import scipy.optimize
from sklearn import svm
import numpy as np
import string
import random
import string
from sklearn import linear_model
from sklearn.metrics import accuracy_score

import warnings
warnings.filterwarnings("ignore")

def assertFloat(x):
    assert type(float(x)) == float

def assertFloatList(items, N):
    assert len(items) == N
    assert [type(float(x)) for x in items] == [float]*N

def readGz(path):
    for l in gzip.open(path, 'rt'):
        yield eval(l)

def readCSV(path):
    f = gzip.open(path, 'rt')
    f.readline()
    for l in f:
        u,b,r = l.strip().split(',')
        r = int(r)
        yield u,b,r

answers = {}

"""## QUESTION 1"""

allRatings = []
for l in readCSV("train_Interactions.csv.gz"):
    allRatings.append(l)

len(allRatings)
allRatings[0]

ratingsTrain = allRatings[:190000]
ratingsValid = allRatings[190000:]
ratingsPerUser = defaultdict(list)
ratingsPerItem = defaultdict(list)
for u,b,r in ratingsTrain:
    ratingsPerUser[u].append((b,r))
    ratingsPerItem[b].append((u,r))

# Copied from baseline code
bookCount = defaultdict(int)
totalRead = 0

for user,book,_ in readCSV("train_Interactions.csv.gz"):
    bookCount[book] += 1
    totalRead += 1

mostPopular = [(bookCount[x], x) for x in bookCount]
mostPopular.sort()
mostPopular.reverse()

return1 = set()
count = 0
for ic, i in mostPopular:
    count += ic
    return1.add(i)
    if count > totalRead/2: break

ratingsValid[:10]

def getNegative(user):
  read_books = set([book for book, _ in ratingsPerUser[user]])
  all_books = list(ratingsPerItem.keys())
  while True:
    negative_book = random.choice(all_books)
    if negative_book not in read_books:
      return negative_book

negative_entries = []
new_valid_dataset = []
for index in range(len(ratingsValid)):
  user = ratingsValid[index][0]
  book = ratingsValid[index][1]
  new_valid_dataset.append((user, book, 1))
  negative_sample = getNegative(user)
  negative_entries.append((user, negative_sample, 0))

len(negative_entries)

new_valid_dataset += negative_entries

len(new_valid_dataset)

new_valid_dataset[:10]

correct = 0
for d in new_valid_dataset:
  if d[1] in return1:
    correct += (d[2] == 1)
  else:
    correct += (d[2] == 0)
acc1 = correct / len(new_valid_dataset)
acc1

answers['Q1'] = acc1
assertFloat(answers['Q1'])
answers['Q1']

"""## QUESTION 2"""

class Baseline:
  def train(self, threshold):
    self.return1 = set()
    count = 0
    for ic, i in mostPopular:
      count += ic
      self.return1.add(i)
      if count > totalRead*threshold: break

  def predict(self, user, book):
    if book in self.return1:
      return 1
    else:
      return 0

max_acc = 0
max_threshold = 0
for threshold in range(1,100):
  baseline = Baseline()
  baseline.train(threshold/100)
  correct = 0
  for d in new_valid_dataset:
    pred = baseline.predict(d[0], d[1])
    correct += d[2] == pred
  acc = correct / len(new_valid_dataset)
  if acc > max_acc:
    max_acc = acc
    max_threshold = threshold

answers['Q2'] = [max_threshold, max_acc]
assertFloat(answers['Q2'][0])
assertFloat(answers['Q2'][1])
answers['Q2']

"""## QUESTION 3"""

def Jaccard(s1, s2):
    numer = len(s1.intersection(s2))
    denom = len(s1.union(s2))
    if denom == 0:
        return 0
    return numer / denom

def compute_max_jaccard(dataset):
  max_jac = {}
  for user,book,_ in dataset:
    users = set([u for u, _ in ratingsPerItem[book]])
    max_sim = 0
    books = set([b for b, _ in ratingsPerUser[user]])
    for b in books:
      users1 = set([u for u, _ in ratingsPerItem[b]])
      sim = Jaccard(users, users1)
      max_sim = max(max_sim, sim)
    max_jac[(user, book)] = max_sim
  return max_jac

max_jac_valid = compute_max_jaccard(new_valid_dataset)

max_acc = 0
max_threshold = 0
for threshold in range(1,1001):
  correct = 0
  for user,book,rating in new_valid_dataset:
    max_sim = max_jac_valid[(user, book)]
    if max_sim > threshold/1000:
      correct += (rating == 1)
    else:
      correct += (rating == 0)
  acc = correct / len(new_valid_dataset)
  if acc > max_acc:
    max_acc = acc
    max_threshold = threshold/1000

max_acc, max_threshold

answers['Q3'] = max_acc
assertFloat(answers['Q3'])
answers['Q3']

"""## QUESTION 4"""

def feat(datum, baseline, max_jac):
  user,book, _ = datum
  max_sim = max_jac[(user, book)]
  return [1, max_sim , baseline.predict(user, book)]

baseline = Baseline()
baseline.train(0.71)
len(baseline.return1)

Xvalid = [feat(d, baseline, max_jac_valid) for d in new_valid_dataset]
yvalid = [d[2] for d in new_valid_dataset]
len(Xvalid), len(yvalid), len(max_jac_valid)

mod = linear_model.LogisticRegression(fit_intercept=False)
mod.fit(Xvalid,yvalid)

pred = mod.predict(Xvalid)

def accuracy(predictions, y):
  return accuracy_score(y, predictions)

mod.coef_

acc4 = accuracy(pred, yvalid)
acc4

answers['Q4'] = acc4
assertFloat(answers['Q4'])
answers['Q4']

"""## QUESTION 5

Uploaded to Assignment 1 gradescope link
"""

predictions = open("predictions_Read.csv", 'w')
test_dataset = []
count=0
for l in open("pairs_Read.csv"):
    if l.startswith("userID"):
        predictions.write(l)
        continue
    u,b = l.strip().split(',')
    test_dataset.append((u,b, -1))
    count+=1
    # (etc.)
max_jac_test = compute_max_jaccard(test_dataset)
Xtest = [feat(d, baseline, max_jac_test) for d in test_dataset]
count

len(Xtest)

pred_test = mod.predict(Xtest)
len(pred_test)

pred[:10]

for i in range(len(pred_test)):
  print(test_dataset[i][0], " ", test_dataset[i][1], " ", pred_test[i])
  predictions.write(str(test_dataset[i][0]) + ',' + str(test_dataset[i][1]) + ',' + str(pred_test[i]) + '\n')

predictions.close()

answers['Q5'] = "I confirm that I have uploaded an assignment submission to gradescope"

assert type(answers['Q5']) == str

"""## QUESTION 6"""

ytrain = [d[2] for d in ratingsTrain]
alpha = sum(ytrain) / len(ytrain)
alpha

reg = 1
lr = 0.01
bu= {}
bi= {}
for u in ratingsPerUser:
    bu[u] = 0
for b in ratingsPerItem:
    bi[b] = 0
epoch = 50
def gradient_descent(reg, alpha, bu, bi, epoch):
  for e in range(epoch):
    sse = 0
    for d in ratingsTrain:
      u,b,r = d
      prediction = alpha + bu[u] + bi[b]
      err = r - prediction
      sse += err**2
      bu[u] += lr*(err - reg*bu[u])
      bi[b] += lr*(err - reg*bi[b])
    l2 = reg * (sum([bu[u]**2 for u in bu]) + sum([bi[b]**2 for b in bi]))
    sse += l2
    # print(f"Epoch {e+1}/{epoch}, SSE: {sse}")
  return sse
gradient_descent(reg, alpha, bu, bi, epoch)

def get_mse(alpha, bu, bi):
  mse = 0
  for user, book, rating in ratingsValid:
    prediction = alpha + bu[user] + bi[book]
    mse += (rating - prediction)**2
  mse /= len(ratingsValid)
  return mse
mse = get_mse(alpha, bu, bi)
mse

answers['Q6'] = mse

assertFloat(answers['Q6'])

"""## QUESTION 7"""

user_beta = [*zip(bu.values(), bu.keys())]
user_beta.sort()
user_beta[:10]

maxUser = user_beta[-1][1]
maxBeta = user_beta[-1][0]
minBeta = user_beta[0][0]
minUser = user_beta[0][1]

answers['Q7'] = [maxUser, minUser, maxBeta, minBeta]
answers['Q7']

assert [type(x) for x in answers['Q7']] == [str, str, float, float]

min_mse = float('inf')
best_l = None
for l in range(30,50):
  l = l/100
  bu= {}
  bi= {}
  for u in ratingsPerUser:
      bu[u] = 0
  for b in ratingsPerItem:
      bi[b] = 0
  gradient_descent(l, alpha, bu, bi, 50)
  mse = get_mse(alpha, bu, bi)
  if mse < min_mse:
    min_mse = mse
    best_l = l
  print(f"Lambda: {l}, MSE: {mse}")

best_l, min_mse

answers['Q8'] = (best_l, min_mse)
answers['Q8']

buf = defaultdict(float)
bif = defaultdict(float)
for u in ratingsPerUser:
    buf[u] = 0
for b in ratingsPerItem:
    bif[b] = 0
epoch = 50
gradient_descent(best_l, alpha, buf, bif, epoch)

"""Uploaded to Assignment 1 gradescope link"""

test_dataset_rating = []
predictions = open("predictions_Rating.csv", 'w')
for l in open("pairs_Rating.csv"):
    if l.startswith("userID"): # header
        predictions.write(l)
        continue
    u,b = l.strip().split(',') # Read the user and item from the "pairs" file and write out your prediction
    test_dataset_rating.append((u,b))
    prediction = alpha + buf[u] + bif[b]
    predictions.write(u + ',' + b + ',' + str(prediction) + '\n')

predictions.close()

f = open("answers_hw3.txt", 'w')
f.write(str(answers) + '\n')
f.close()
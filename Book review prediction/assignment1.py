# -*- coding: utf-8 -*-
"""assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14UqY_Sql1k78CQ3ODzroIx2qHrNqU46-

### ASSIGNMENT 1
"""

import gzip
from collections import defaultdict
import math
import scipy.optimize
from sklearn import svm
import numpy as np
import string
import random
import string
from sklearn.model_selection import KFold
from sklearn import linear_model
from sklearn.metrics import accuracy_score
from scipy.spatial.distance import cosine
from scipy.stats import pearsonr

def readGz(path):
  for l in gzip.open(path, 'rt'):
    yield eval(l)

def readCSV(path):
  f = gzip.open(path, 'rt')
  f.readline()
  for l in f:
    yield l.strip().split(',')

allRatings = []
for l in readCSV("train_Interactions.csv.gz"):
  allRatings.append(l)

#Get popular books

bookCount = defaultdict(int)
totalRead = 0

for user,book,_ in readCSV("train_Interactions.csv.gz"):
    bookCount[book] += 1
    totalRead += 1

mostPopular = [(bookCount[x], x) for x in bookCount]
mostPopular.sort()
mostPopular.reverse()

ratingsTrain = allRatings[:190000]
ratingsValid = allRatings[190000:]

ratingsPerUser = defaultdict(list)
ratingsPerItem = defaultdict(list)
for u,b,r in ratingsTrain:
    ratingsPerUser[u].append((b,r))
    ratingsPerItem[b].append((u,r))
all_books = set(ratingsPerItem.keys())

"""Generating negative samples"""

class Popular:
  def train(self, threshold):
    self.return1 = set()
    count = 0
    for ic, i in mostPopular:
      count += ic
      self.return1.add(i)
      if count > totalRead*threshold: break

  def predict(self, user, book):
    if book in self.return1:
      return 1
    else:
      return 0

"""# **READ PREDICTIONS**"""

popular = Popular()
popular.train(0.73)

validation_set = []
for user, book, rating in ratingsValid:
  read_books = set([book for book, _ in ratingsPerUser[user]])
  negative_sample = random.choice(list(all_books - read_books - popular.return1))
  validation_set.append((user, negative_sample, 0))
  if int(rating) > 0:
    validation_set.append((user, book, 1))
  else:
    validation_set.append((user, book, 0))

len(validation_set)

# To get best threshold for popularity
# max_acc = 0
# max_threshold = 0
# for threshold in range(1,100):
#   baseline = Popular()
#   baseline.train(threshold/100)
#   correct = 0
#   for d in new_training_dataset:
#     pred = baseline.predict(d[0], d[1])
#     correct += d[2] == pred
#   acc = correct / len(new_training_dataset)
#   if acc > max_acc:
#     max_acc = acc
#     max_threshold = threshold
# max_acc, max_threshold

"""Calculate cosine and pearson similarity"""

def cp_similarity(i1, i2):
    users1 = set(user for user, _ in ratingsPerItem[i1])
    users2 = set(user for user, _ in ratingsPerItem[i2])
    common_users = users1.intersection(users2)

    if len(common_users) < 2:
        return 0, 0

    vector1 = [float(rating) for user, rating in ratingsPerItem[i1] if user in common_users]
    vector2 = [float(rating) for user, rating in ratingsPerItem[i2] if user in common_users]

    # Cosine Similarity
    if np.linalg.norm(vector1) == 0 or np.linalg.norm(vector2) == 0:
        cosine_sim = 0
    else:
        cosine_sim = 1 - cosine(vector1, vector2)

    # Pearson Correlation
    if len(set(vector1)) == 1 or len(set(vector2)) == 1:
        pearson_sim = 0
    else:
        correlation, _ = pearsonr(vector1, vector2)
        pearson_sim = correlation if not np.isnan(correlation) else 0

    return cosine_sim, pearson_sim

def Jaccard(i1, i2):
    users1 = set(user for user, _ in ratingsPerItem[i1])
    users2 = set(user for user, _ in ratingsPerItem[i2])
    numer = len(users1.intersection(users2))
    denom = len(users1.union(users2))
    if denom == 0:
        return 0
    return numer / denom

def getSimilarities(user, book):
    if book not in ratingsPerItem or user not in ratingsPerUser:
        return 0, 0, 0
    max_j = 0
    max_p = 0
    max_c = 0
    books = set([b for b, _ in ratingsPerUser[user]])
    for b in books:
        if b == book:
            continue
        cos_sim, p_sim = cp_similarity(book, b)
        jac_sim = Jaccard(book, b)

        if not np.isnan(cos_sim) and not np.isclose(cos_sim, 0):
            max_c = max(max_c, cos_sim)
        if not np.isnan(p_sim) and not np.isclose(p_sim, 0):
            max_p = max(max_p, p_sim)
        if not np.isnan(jac_sim) and not np.isclose(jac_sim, 0):
            max_j = max(max_j, jac_sim)

    return max_c, max_p, max_j

def feat(datum, popular):
  user,book = datum
  max_jac, max_cos, max_pearson = getSimilarities(user, book)
  return [1, max_jac, max_cos, max_pearson , popular.predict(user, book)]

"""Hyperparameters tuned using k-fold cross validation"""

# Checked the best threshold and C value using k-fold
# best_accuracy = 0
# best_threshold = 0
# best_C = 0
# # best_model = None

# # Try different threshold values
# for threshold in threshold_values:
#     popular_model = Popular()
#     popular_model.train(threshold)
#     for C_value in C_values:
#       fold_accuracies = []
#       for train_index, test_index in kf.split(X):
#           X_train, X_test = X[train_index], X[test_index]
#           y_train, y_test = y[train_index], y[test_index]

#           # Train Popular model with the current threshold


#           max_jac_train = compute_max_jaccard(X_train)
#           max_jac_test = compute_max_jaccard(X_test)

#           X_train_feat = [feat((user, book), popular_model, max_jac_train) for user, book in X_train]
#           X_test_feat = [feat((user, book), popular_model, max_jac_test) for user, book in X_test]

#           model = linear_model.LogisticRegression(C=C_value, fit_intercept=False)
#           model.fit(X_train_feat, y_train)

#           accuracy = model.score(X_test_feat, y_test)
#           fold_accuracies.append(accuracy)

#       avg_acc = np.mean(fold_accuracies)
#       print(f"Threshold {threshold}, C-value {C_value} - Average Accuracy: {avg_acc}")

#       if avg_acc > best_accuracy:
#           best_accuracy = avg_acc
#           best_threshold = threshold
#           best_C = C_value

# print(f"Best Threshold: {best_threshold}, Best Accuracy: {best_accuracy}")

Xtrain = [feat((u,b), popular) for u,b, r in validation_set]
ytrain = [r for u,b, r in validation_set]

predictions = open("predictions_Read.csv", 'w')
test_dataset = []
for l in open("pairs_Read.csv"):
    if l.startswith("userID"):
        predictions.write(l)
        continue
    u,b = l.strip().split(',')
    test_dataset.append((u,b))
Xtest = [feat(d, popular) for d in test_dataset]

mod = linear_model.LogisticRegression(C=100, class_weight= 'balanced', fit_intercept=False)
mod.fit(Xtrain,ytrain)

pred = mod.predict(Xtest)
pred2 = mod.predict(Xtrain)

mod.coef_

def accuracy(predictions, y):
  return accuracy_score(y, predictions)

accuracy(pred2, ytrain)

temp = 0
for i in range(len(pred)):
  if pred[i] == 1:
    temp+=1
  predictions.write(str(test_dataset[i][0]) + ',' + str(test_dataset[i][1]) + ',' + str(pred[i]) + '\n')
temp

predictions.close()

"""# **RATING PREDICTION**"""

pip install scikit-surprise

from surprise import Dataset, Reader
from surprise.model_selection import train_test_split
import pandas as pd
import numpy as np
from collections import defaultdict
from surprise import SVD
from surprise import accuracy
from surprise.model_selection import train_test_split
from surprise.model_selection import GridSearchCV

reader = Reader(rating_scale=(0, 5))

data = Dataset.load_from_df(
    pd.DataFrame(allRatings, columns=["user_id", "item_id", "rating"]),
    reader=reader
)

trainset, validset = train_test_split(data, test_size=0.2)

# param_grid = {
#     "n_factors": [1, 3, 5, 10, 15],
#     "lr_all": [0.005, 0.01, 0.02],
#     "reg_all": [0.1, 0.2, 0.3, 0.4],
#     "n_epochs" : [30, 40, 50]
# }

# grid_search = GridSearchCV(SVD, param_grid, measures=["rmse"], cv=3)
# grid_search.fit(data)

# print(grid_search.best_params["rmse"])
# print(grid_search.best_score["rmse"])

# best_svd = grid_search.best_estimator["rmse"]
# best_svd.fit(trainset)

# predictions = best_svd.test(validset)
# valid_mse = accuracy.mse(predictions, verbose=True)

# used the hyperparameters of the best_svd model which were found using grid search
svd_model = SVD(
    n_factors=10,
    reg_all=0.2,
    lr_all=0.005,
    n_epochs=40
)
svd_model.fit(data.build_full_trainset())

test_dataset_rating = []
predictions = open("predictions_Rating.csv", 'w')
for l in open("pairs_Rating.csv"):
    if l.startswith("userID"): # header
        predictions.write(l)
        continue
    u,b = l.strip().split(',')
    test_dataset_rating.append((u,b))
    predicted_rating = svd_model.predict(u, b)
    predictions.write(u + ',' + b + ',' + str(predicted_rating.est) + '\n')

predictions.close()

"""METHOD - 2 (using gradient descent)"""

# ytrain = [int(d[2]) for d in ratingsTrain]
# Xtrain = [(d[0], d[1]) for d in ratingsTrain]
# alpha = sum(ytrain) / len(ytrain)
# alpha

# # Initialize variables
# # reg_biass = [0.2, 0.23, 0.26, 0.3, 0.32, 0.35, 0.37, 0.4, 0.42, 0.44]  # Regularization for biases
# reg_bias = 0.23
# reg_latent = 0.9  # Regularization for latent factors
# lr = 0.01  # Learning rate
# latent_factors = 1  # Number of latent factors
# epoch = 50  # Maximum number of epochs
# early_stopping_patience = 5  # Stop training if no improvement for this many epochs

# # Prepare initial parameters
# alpha = sum(int(d[2]) for d in ratingsTrain) / len(ratingsTrain)  # Global average rating
# bu = defaultdict(float)  # User biases
# bi = defaultdict(float)  # Item biases
# # p = {u: np.random.randn(latent_factors) for u in ratingsPerUser}  # User latent factors
# # q = {b: np.random.randn(latent_factors) for b in ratingsPerItem}  # Item latent factors

# # Function to calculate MSE on a dataset
# def calculate_mse(data, alpha, bu, bi, p, q):
#     mse = 0
#     for u, b, r in data:
#         r = float(r)
#         prediction = alpha + bu.get(u, 0) + bi.get(b, 0) + np.dot(p.get(u, np.zeros(latent_factors)), q.get(b, np.zeros(latent_factors)))
#         mse += (r - prediction) ** 2
#     mse /= len(data)
#     return mse

# # Gradient descent with validation and early stopping
# def gradient_descent_with_validation(reg_bias, reg_latent, alpha, bu, bi, p, q, epoch, ratingsTrain, ratingsValid):
#     best_mse = float('inf')  # Best validation MSE
#     patience_counter = 0  # Counter for early stopping

#     for e in range(epoch):
#         train_mse = 0
#         for u, b, r in ratingsTrain:
#             r = float(r)
#             prediction = alpha + bu[u] + bi[b] + np.dot(p[u], q[b])
#             err = r - prediction
#             train_mse += err ** 2

#             # Update biases
#             bu[u] += lr * (err - reg_bias * bu[u])
#             bi[b] += lr * (err - reg_bias * bi[b])

#             # Update latent factors
#             p[u] += lr * (err * q[b] - reg_latent * p[u])
#             q[b] += lr * (err * p[u] - reg_latent * q[b])

#         # Calculate regularization penalty
#         l2 = reg_bias * (sum(bu[u] ** 2 for u in bu) + sum(bi[b] ** 2 for b in bi)) \
#             + reg_latent * (sum(np.sum(p[u] ** 2) for u in p) + sum(np.sum(q[b] ** 2) for b in q))

#         # Average train MSE and add regularization penalty
#         train_mse /= len(ratingsTrain)
#         train_mse += l2

#         # Calculate validation MSE
#         valid_mse = calculate_mse(ratingsValid, alpha, bu, bi, p, q)

#         print(f"Epoch {e+1}/{epoch}, Train MSE: {train_mse:.4f}, Validation MSE: {valid_mse:.4f}")

#         # Early stopping logic
#         if valid_mse < best_mse:
#             best_mse = valid_mse
#             patience_counter = 0  # Reset patience counter
#         else:
#             patience_counter += 1
#             if patience_counter >= early_stopping_patience:
#                 print("Early stopping triggered.")
#                 break

#     return bu, bi, p, q, valid_mse

# # Train the model
# # min_mse = 1000
# # best_reg = 0
# # for reg in reg_biass:
# #     reg_bias = reg
# #     reg_latent = 0.9
# #     alpha = sum(int(d[2]) for d in ratingsTrain) / len(ratingsTrain)  # Global average rating
# #     bu = defaultdict(float)  # User biases
# #     bi = defaultdict(float)  # Item biases
# #     p = {u: np.random.randn(latent_factors) for u in ratingsPerUser}  # User latent factors
# #     q = {b: np.random.randn(latent_factors) for b in ratingsPerItem}
# #     bu, bi, p, q, valid_mse = gradient_descent_with_validation(reg_bias, reg_latent, alpha, bu, bi, p, q, epoch, ratingsTrain, ratingsValid)
# #     if valid_mse < min_mse:
# #         min_mse = valid_mse
# #         best_reg = reg
# # best_reg
# bu, bi, p, q, valid_mse = gradient_descent_with_validation(reg_bias, reg_latent, alpha, bu, bi, p, q, epoch, ratingsTrain, ratingsValid)

# test_dataset_rating = []
# predictions = open("predictions_Rating.csv", 'w')
# for l in open("pairs_Rating.csv"):
#     if l.startswith("userID"): # header
#         predictions.write(l)
#         continue
#     u,b = l.strip().split(',') # Read the user and item from the "pairs" file and write out your prediction
#     test_dataset_rating.append((u,b))
#     # predicted_rating = svd_model.predict(u, b)
#     # print(predicted_rating.est)
#     prediction = alpha + bu.get(u, 0) + bi.get(b, 0) + np.dot(p.get(u, np.zeros(latent_factors)), q.get(b, np.zeros(latent_factors)))
#     # predictions.write(u + ',' + b + ',' + str(predicted_rating.est) + '\n')
#     predictions.write(u + ',' + b + ',' + str(prediction) + '\n')

# predictions.close()

